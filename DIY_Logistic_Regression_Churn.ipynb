{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f3d6ed-9307-467a-83c1-84997e3c26ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix,ConfusionMatrixDisplay, precision_recall_fscore_support, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dc8dcb-2474-4c83-9533-8a7bc64638ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# also set a random state\n",
    "rs = 123"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a907f574-bc36-44e0-a761-70bc192328f6",
   "metadata": {},
   "source": [
    "## **1. Exploratory Data Analysis(EDA) and Feature Engineering**\n",
    "Before we get to the model implementation, it is essential to examine the dataset and carefully select the features that will serve as inputs for the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd804009-bbb1-492e-aa0b-3e0c5dab24b2",
   "metadata": {},
   "source": [
    "### Load and explore the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91382f14-8a24-47ae-8c60-bc782201b7db",
   "metadata": {},
   "source": [
    "First, let's load the dataset as a `Pandas` dataframe and conduct some basic EDA tasks on it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecd8ca8-bf25-4001-8c9c-1174798f7ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset_path = \"https://raw.githubusercontent.com/andresmorenoviteri/ML-models/main/telecom_customer_churn.csv\"\n",
    "churn_df = pd.read_csv(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6b2e35-f6e5-4a3b-aceb-20cf67082ae8",
   "metadata": {},
   "source": [
    "And, let's quickly check its column types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e1d137-e748-4df7-b38f-630a5d9d08c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df.____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2908718-bba6-43ec-aa74-f9fe1c9797e2",
   "metadata": {},
   "source": [
    "Print the first ten customers items:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f449a552-2200-423d-9375-b2cb3de5a5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df.head(____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop `Churn Category` and `Churn Reason`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df = churn_df.drop([____], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a2b19f-df93-44da-9e25-4cc1f4110069",
   "metadata": {},
   "source": [
    "Obtain descriptive statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607824c4-3061-454d-b43e-a3550049dd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df.____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5988d8-3f41-4c8c-9eab-3348c31ec847",
   "metadata": {},
   "source": [
    "Next, let's check the target variable in the `Customer Status` column to see the label values and their distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda24881-4747-4d20-a168-c08969e6eda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df[____].____()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Feature Engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep only the rows that don't contain `Joined` in `Customer Status`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your code here\n",
    "churn_df = ____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that `Customer Status` only has 2 types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df['Customer Status']._____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an **object_df** where the only data types are 'object': "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_df = churn_df.select_dtypes(include=[____])\n",
    "object_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove columns that are probably not helpful like: `Customer ID`, `Gender`, `Married`, `City`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_df = ____\n",
    "object_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the rows that have nan values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only the columns with nan values \n",
    "object_df.loc[:, object_df.isna().any()].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For objects, replace nan values with mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list from column names\n",
    "null_objects = [____]\n",
    "\n",
    "# create for-loop to replace with mode\n",
    "for col in null_objects:\n",
    "    object_df[col].fillna(object_df[col].____[0], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a **targets** variable with `Customer Status` and **object_df** without `Customer Status`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = ____\n",
    "object_df = _____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the names to numbers for our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_object_df = pd.get_dummies(object_df, columns=object_df.columns.to_list(), drop_first=True)\n",
    "encoded_object_df[encoded_object_df.columns.to_list()] = encoded_object_df[encoded_object_df.columns.to_list()].astype(int)\n",
    "encoded_object_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an **churn_df_numeric** where the only data types are 'number': "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df_numeric = ____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only the columns with nan values \n",
    "churn_df_numeric.____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace nan values with average\n",
    "null_numeric = [____]\n",
    "\n",
    "for col in null_numeric:\n",
    "    churn_df_numeric[col].fillna(____.mean(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the two dataframes\n",
    "clean_churn_data = pd.concat([____, ____], axis=1)\n",
    "clean_churn_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af27fff3-1b59-41b6-8d8a-98078cbf5a5f",
   "metadata": {},
   "source": [
    "Create a barplot to see how the labels are distributed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_churn_data['Customer Status'].value_counts().____.____(color=[____])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's process the raw dataset and construct input data `X` and label/output `y` for logistic regression model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92f7ffb-e580-4268-a608-263148c40f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_raw = ____\n",
    "y_raw = ____\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(y_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6028199-895b-4e8b-9131-f24b109ea8ef",
   "metadata": {},
   "source": [
    "All feature columns are now numeric so we just need to scale them. Here we use the `MinMaxScaler` provided by `sklearn` for scaling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9f15c1-8f9d-4157-ad5b-4ae330135835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a MinMaxScaler object\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70fdd68-aae1-427a-830e-22d24cab9cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the raw input features\n",
    "x = scaler.fit_transform(____)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e31a983-9e29-40d6-9644-7ce36b03d12a",
   "metadata": {},
   "source": [
    "Let's check the scaled feature value range:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9e0131-97ba-476d-8275-ca7b1ea749a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The range of feature inputs are within {x.min()} to {x.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e484528-db33-4f3f-9f1d-79b5a8172ead",
   "metadata": {},
   "source": [
    "## **3. Train logistic regression models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f5376b-3349-4371-9555-acdf21f4f198",
   "metadata": {},
   "source": [
    "First, let's split the dataset into a training and a testing dataset. Training dataset will be used to train and (maybe) tune models, and testing dataset will be used to evaluate the models. Note that you may also split the training dataset into train and validation sets where the validation dataset is only used to tune the model and to set the model parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80e6758-8396-4681-b25e-c03b76e1032e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's split the training and testing dataset\n",
    "x_train, x_test, y_train, y_test = train_test_split(____, ____, ____ stratify=y, random_state = ____)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41679ec3-cafb-424f-ad24-0be82e383d30",
   "metadata": {},
   "source": [
    "Let's look at the shapes of the split datasets:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192e093d-c561-4bc4-b60d-09bb5ad439f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training dataset shape, x_train: {x_train.shape}, y_train: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6873f5-bfce-490b-8390-62b51812c6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Testing dataset shape, x_test: {x_test.shape}, y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4693b8a0-db8c-4307-bcce-c9be2778e02a",
   "metadata": {},
   "source": [
    "OK, now we have the training and testing datasets ready, let's start the model training task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d747019-0f7f-489d-a7b6-1d9598c7c9b6",
   "metadata": {},
   "source": [
    "We first define a `sklearn.linear_model.LogisticRegression` model with the following arguments, you can check the comment for each argument for what it means.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115f3773-ff80-4553-9e3e-1fd9135a524b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 penalty to shrink coefficients without removing any features from the model\n",
    "# penalty= 'l2'\n",
    "# Use lbfgs for L2 penalty and multinomial classes\n",
    "# solver = 'lbfgs'\n",
    "# Max iteration = 1000\n",
    "max_iter = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97274b82-6d99-4eb4-88a3-c1e4318c621e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a logistic regression model with above arguments\n",
    "l2_model = LogisticRegression(random_state=rs, max_iter=max_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474e5114-e4d4-4f6d-91c1-ea15eb3779d0",
   "metadata": {},
   "source": [
    "Let's train the model with training input data `X_train` and labels `y_train`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673035b3-97d3-4c77-992a-3d54cd8a238e",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_model.____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd536d2-0dba-414c-a241-00da5a3cdfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_preds = ____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea361f7-8449-4ad5-93c3-8ee070563b0a",
   "metadata": {},
   "source": [
    "Because we may need to evaluate the model multiple times with different model hyper parameters, here we define an utility method to take the ground truths `y_test` and the predictions `preds`, and return a Python `dict` with `accuracy`, `recall`, `precision`, and `f1score`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3770eb92-a2fc-42cf-9da7-b44f18f0af1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(yt, yp):\n",
    "    results_pos = {}\n",
    "    results_pos['accuracy'] = accuracy_score(yt, yp)\n",
    "    precision, recall, f_beta, _ = precision_recall_fscore_support(yt, yp)\n",
    "    results_pos['recall'] = recall\n",
    "    results_pos['precision'] = precision\n",
    "    results_pos['f1score'] = f_beta\n",
    "    return results_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8328bb8-76e8-4fce-af39-4a07c4120c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_metrics(y_test, l2_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = confusion_matrix(y_test, l2_preds, normalize='true')\n",
    "sns.set_context('talk')\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cf,display_labels=l2_model.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca5efc8-64fa-4def-a16e-f950ba62f417",
   "metadata": {},
   "source": [
    "As we can see from  the above evaluation results, the logistic regression model has relatively good performance on this binomial classification task. The overall accuracy is around `0.86` and the f1score is around `0.82`. Note that for `recall`, `precision`, and `f1score`, we output the values for each class to see how the model performs on an individual class. And, we can see from the results, the recall for `class=0` (Churned) is only ok. This is actually a common problem called imbalanced classification challenge. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a1175e-e296-4e10-bef5-41fa1393954c",
   "metadata": {},
   "source": [
    "Next, let's try defining another logistic regression model with l1 penality this time, to see if our classification performance would be improved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02cfc8f-8381-4237-add7-fe7dfe8a45ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 penalty to shrink coefficients without removing any features from the model\n",
    "penalty= 'l1'\n",
    "# Use saga for L1 penalty and multinomial classes\n",
    "solver = 'saga'\n",
    "# Max iteration = 1000\n",
    "max_iter = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4dfc78-5fd5-439f-a01c-107d5ca4b1b0",
   "metadata": {},
   "source": [
    "Then we define another logistic regression model with above arguments using l1 penality and related solver.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00344cf-66f2-4f11-87af-6d8434b91b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a logistic regression model with above arguments\n",
    "l1_model = LogisticRegression(random_state=____, penalty=____, solver=____, max_iter = ____)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a1c017-51bf-4587-bd95-3a28167ce932",
   "metadata": {},
   "source": [
    "We can start to train the new `l1_model` with the new taining dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4a6c7a-9278-4c7b-8fd7-52b3eacc3f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_model.____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b487a9-03b5-4275-8641-8ef56baf0fa0",
   "metadata": {},
   "source": [
    "And, make predictions using the input in the test dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84aa7c27-3b23-4616-8790-f88e6b8bc0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_preds = ____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d78729f-d448-44cb-baf3-b4e5c8e86f8a",
   "metadata": {},
   "source": [
    "We can also check the class probability distribution using the `predict_proba` function. For example, we want to see the probabilities of belonging to each class for the first instance in the test dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe968108-8b20-42a9-ad60-471cbf3e02e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "odd_ratios = l1_model.predict_proba(x_test[:1, :])[0]\n",
    "odd_ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8d9b37-d831-4680-a46a-852324c63a28",
   "metadata": {},
   "source": [
    "We can see that  Class 1 has the largest probability 0.96. As such, the model prediction for this instance will be class `1` and this is the same as the `predict` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637db0d3-2abb-4df1-835f-4db9599bf196",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_model.predict(x_test[:1, :])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1065ae07-fcf8-4b34-a842-bf41a8240ce2",
   "metadata": {},
   "source": [
    "Given the true labels (`y_test`) and predictions, we can evaluate the model performance by calling the utility `evaluate_metrics`  method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242b6412-b3a7-441f-9e64-3b48b391bea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_metrics(____)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18eb4c1-5fe9-4fea-93fd-7ec101c93bdd",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7e8847-e9b1-41aa-9b2a-f053b173c283",
   "metadata": {},
   "source": [
    "We can also plot the confusion matrix based on the true labels and predictions using the `confusion_matrix` method provided by `sklearn`,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd8f0a9-dd88-4e4d-a8a2-c9864d2d9f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = confusion_matrix(y_test, l1_preds, normalize='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9019f89b-4991-4e9b-8abe-9785e234bee8",
   "metadata": {},
   "source": [
    "and easily visualize it using a heatmap method provided by `seaborn`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861d6fa6-7890-46bb-b561-59650d78bee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cf,display_labels=l1_model.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc753e7-5ce4-4c0c-8a1c-41ba92fad8cb",
   "metadata": {},
   "source": [
    "### Interpret logistic regression models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0ea663-1f6c-4a25-a146-d25d41f27a43",
   "metadata": {},
   "source": [
    "One way to interpret logistic regression models is by analyzing feature coefficients. Although it may not be as effective as the regular linear regression models because the logistic regression model has a sigmoid function, we can still get a sense for the importance or impact of each feature.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c451423-92de-4753-a3bd-adbcfae24976",
   "metadata": {},
   "source": [
    "We can check the coefficients for logistic regression model using its `coef_` attribute:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fe8264-22e9-4ea9-87c4-f32945eda53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67c4a57-ad77-43d7-b335-20e92df86072",
   "metadata": {},
   "source": [
    "The `coef_` is a coefficients list with three elements, one element is the actual coefficent for class 0, 1, 2. To better analyze the coefficients, let's use three utility methods to sort and visualize them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439f4fcc-7f6c-4e28-94a9-35f570cec8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and sort feature coefficients\n",
    "def get_feature_coefs(regression_model, label_index, columns):\n",
    "    coef_dict = {}\n",
    "    for coef, feat in zip(regression_model.coef_[label_index, :], columns):\n",
    "        if abs(coef) >= 0.01:\n",
    "            coef_dict[feat] = coef\n",
    "    # Sort coefficients\n",
    "    coef_dict = {k: v for k, v in sorted(coef_dict.items(), key=lambda item: item[1])}\n",
    "    return coef_dict\n",
    "\n",
    "# Generate bar colors based on if value is negative or positive\n",
    "def get_bar_colors(values):\n",
    "    color_vals = []\n",
    "    for val in values:\n",
    "        if val <= 0:\n",
    "            color_vals.append('r')\n",
    "        else:\n",
    "            color_vals.append('g')\n",
    "    return color_vals\n",
    "\n",
    "# Visualize coefficients\n",
    "def visualize_coefs(coef_dict):\n",
    "    features = list(coef_dict.keys())\n",
    "    values = list(coef_dict.values())\n",
    "    y_pos = np.arange(len(features))\n",
    "    color_vals = get_bar_colors(values)\n",
    "    plt.rcdefaults()\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.barh(y_pos, values, align='center', color=color_vals)\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(features)\n",
    "    # labels read top-to-bottom\n",
    "    ax.invert_yaxis()  \n",
    "    ax.set_xlabel('Feature Coefficients')\n",
    "    ax.set_title('')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = clean_churn_data.columns[clean_churn_data.columns != 'Customer Status']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446f04b4-6157-47df-9fc4-7d52a4c1b903",
   "metadata": {},
   "source": [
    "Then, let's visualize the sorted coefficient for class 0, the `Churned` class: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64657e44-12e3-4af4-9996-58b88cfc4491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the coefficents for Class 0, Stayed\n",
    "coef_dict = get_feature_coefs(l1_model, 0, feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d5397f-5585-492e-b8ea-4e5b4ff86582",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_coefs(coef_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893cea90-80cd-4720-afa9-180e58e31145",
   "metadata": {},
   "source": [
    "As we can see, Tenure in Months, Number of Dependents, Number of Referrals, Contract, and Phone Service huave high positive coefficients. Total Charges, Monthly Charges, Maried and Age will most likely not influence a churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=200,       # number of trees\n",
    "    max_depth=1,            # max depth of each tree\n",
    "    learning_rate=0.1,      # step size shrinkage\n",
    "    random_state=42,\n",
    "    eval_metric='logloss'   # evaluation metric\n",
    ")\n",
    "xgb_model.fit(x_train, y_train)\n",
    "\n",
    "# predict\n",
    "y_pred = xgb_model.predict(x_test)\n",
    "\n",
    "# evaluate\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "sns.set_context('talk')\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cf,display_labels=xgb_model.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch Venv",
   "language": "python",
   "name": "torch_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "prev_pub_hash": "650750a7c481003d0f489aa2ab268ed3c6e79f8c71e9359d815bdee7f137b02f"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
